{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, psycopg2\n",
    "from psycopg2.extensions import AsIs\n",
    "\n",
    "# Connect to Postgres\n",
    "with open('./credentials', 'r') as credential_yaml:\n",
    "    credentials = yaml.load(credential_yaml)\n",
    "\n",
    "with open('./config', 'r') as config_yaml:\n",
    "    config = yaml.load(config_yaml)\n",
    "\n",
    "    \n",
    "snorkel_connection = psycopg2.connect(\n",
    "    dbname=credentials['snorkel_postgres']['database'],\n",
    "    user=credentials['snorkel_postgres']['user'],\n",
    "    password=credentials['snorkel_postgres']['password'],\n",
    "    host=credentials['snorkel_postgres']['host'],\n",
    "    port=credentials['snorkel_postgres']['port'])\n",
    "snorkel_cursor = snorkel_connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS candidate CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS context CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS document CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS feature CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS feature_key CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS gold_label CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS gold_label_key CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS label CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS label_key CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS marginal CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS prediction CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS prediction_key CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS sentence CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS span CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS spouse CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS stable_label CASCADE;\n",
    "\"\"\")\n",
    "\n",
    "snorkel_connection.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "###\n",
    "os.environ['SNORKELDB'] = 'postgres://BKumpf@localhost:5432/snorkel'\n",
    "\n",
    "###\n",
    "from snorkel import SnorkelSession\n",
    "session = SnorkelSession()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "# Connect to Postgres\n",
    "with open('./credentials', 'r') as credential_yaml:\n",
    "    credentials = yaml.load(credential_yaml)\n",
    "\n",
    "with open('./config', 'r') as config_yaml:\n",
    "    config = yaml.load(config_yaml)\n",
    "\n",
    "# Connect to Postgres\n",
    "connection = psycopg2.connect(\n",
    "    dbname=credentials['postgres']['database'],\n",
    "    user=credentials['postgres']['user'],\n",
    "    password=credentials['postgres']['password'],\n",
    "    host=credentials['postgres']['host'],\n",
    "    port=credentials['postgres']['port'])\n",
    "cursor = connection.cursor()\n",
    "\n",
    "snorkel_connection = psycopg2.connect(\n",
    "    dbname=credentials['snorkel_postgres']['database'],\n",
    "    user=credentials['snorkel_postgres']['user'],\n",
    "    password=credentials['snorkel_postgres']['password'],\n",
    "    host=credentials['snorkel_postgres']['host'],\n",
    "    port=credentials['snorkel_postgres']['port'])\n",
    "snorkel_cursor = snorkel_connection.cursor()\n",
    "\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT DISTINCT(docid) FROM evap_sentences_nlp352;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "count = 1\n",
    "for docid in cursor:\n",
    "    snorkel_cursor.execute(\"INSERT INTO context (id, type, stable_id) VALUES (nextval('context_id_seq'), 'document', %(stable_id)s)\", {\"stable_id\": docid[0] + \"::document:0:0\"})\n",
    "    snorkel_cursor.execute(\"INSERT INTO document (id, name) VALUES (currval('context_id_seq'), %(docid)s)\", {\"count\" : count, \"docid\": docid[0]})\n",
    "    snorkel_connection.commit()\n",
    "    count += 1\n",
    "\n",
    "#IMPORT THE SENTENCES DUMP\n",
    "cursor.execute(\"\"\"\n",
    "            SELECT docid, sentid, words, poses, ners, lemmas, dep_paths, dep_parents\n",
    "            FROM %(my_app)s_sentences_%(my_product)s ORDER BY docid, sentid;\n",
    "            \"\"\", {\n",
    "                \"my_app\": AsIs(config['app_name']),\n",
    "                    \"my_product\": AsIs(config['product'].lower())\n",
    "                    })\n",
    "\n",
    "# Need to get document-level offsets for stable_id at the sentence level.\n",
    "count = 1\n",
    "\n",
    "doc_char_counts = {}\n",
    "\n",
    "for sent in cursor:\n",
    "    parsed_sent = {}\n",
    "    snorkel_cursor.execute(\"SELECT id FROM document WHERE name=%(docid)s\", {\"docid\" : sent[0]})\n",
    "    document_id = snorkel_cursor.fetchone()[0]\n",
    "    parsed_sent[\"document_id\"] = document_id\n",
    "    parsed_sent[\"position\"] = sent[1]\n",
    "    parsed_sent[\"words\"] = sent[2]\n",
    "    parsed_sent[\"pos_tags\"] = sent[3]\n",
    "    parsed_sent[\"ner_tags\"] = sent[4]\n",
    "    parsed_sent[\"lemmas\"] = sent[5]\n",
    "    parsed_sent[\"dep_labels\"] = sent[6]\n",
    "    parsed_sent[\"dep_parents\"] = sent[7]\n",
    "    parsed_sent[\"text\"] = \" \".join(word for word in parsed_sent[\"words\"])\n",
    "    parsed_sent[\"char_offsets\"] = [0 for i in range(len(parsed_sent[\"words\"]))]\n",
    "    parsed_sent[\"abs_char_offsets\"] = [0 for i in range (len(parsed_sent[\"words\"]))]\n",
    "    parsed_sent[\"entitiy_cids\"] = ['O']\n",
    "\n",
    "        \n",
    "    sentence_running_count = 0\n",
    "    for wordidx in range(len(parsed_sent[\"words\"])):\n",
    "        parsed_sent[\"char_offsets\"][wordidx] = sentence_running_count\n",
    "        sentence_running_count += len(parsed_sent[\"words\"][wordidx]) + 1\n",
    "\n",
    "    # This will probably be off by one...\n",
    "    if sent[0] in doc_char_counts:\n",
    "        sentence_start = doc_char_counts[sent[0]] + 1\n",
    "        doc_char_counts[sent[0]] += sentence_running_count\n",
    "    else:\n",
    "        sentence_start = 0\n",
    "        doc_char_counts[sent[0]] = sentence_running_count\n",
    "\n",
    "    # keep this running count as the sentence-level offset stable_id\n",
    "    snorkel_cursor.execute(\"INSERT INTO context (id, type, stable_id) VALUES (nextval('context_id_seq'), 'sentence', %(stable_id)s)\", {\"stable_id\": sent[0] + \"::sentence:%s:%s\" % (sentence_start, doc_char_counts[sent[0]])})\n",
    "\n",
    "    snorkel_connection.commit()\n",
    "    snorkel_cursor.execute(\" \\\n",
    "        INSERT INTO sentence (id, document_id, position, words, pos_tags, ner_tags, lemmas, dep_labels, dep_parents, char_offsets, abs_char_offsets, text) VALUES \\\n",
    "                (currval('context_id_seq'), \\\n",
    "                %(document_id)s, \\\n",
    "                %(position)s, \\\n",
    "                %(words)s, \\\n",
    "                %(pos_tags)s, \\\n",
    "                %(ner_tags)s,  \\\n",
    "                %(lemmas)s, \\\n",
    "                %(dep_labels)s, \\\n",
    "                %(dep_parents)s, \\\n",
    "                %(char_offsets)s, \\\n",
    "                %(abs_char_offsets)s, \\\n",
    "                %(text)s);\", parsed_sent)\n",
    "    snorkel_connection.commit()\n",
    "    count += 1\n",
    "\n",
    "snorkel_cursor.close()\n",
    "snorkel_connection.close()\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Documents:', 296L)\n",
      "('Sentences:', 206435L)\n"
     ]
    }
   ],
   "source": [
    "from snorkel.models import Document, Sentence\n",
    "\n",
    "print(\"Documents:\", session.query(Document).count())\n",
    "print(\"Sentences:\", session.query(Sentence).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Candidate schema\n",
    "\n",
    "#### We now define the schema of the relation mention we want to extract (which is also the schema of the candidates). This must be a subclass of Candidate, and we define it using a helper function. Here we'll define a binary spouse relation mention which connects two Span objects of text. Note that this function will create the table in the database backend if it does not exist:\n",
    "\n",
    "## Evaporite schema\n",
    "\n",
    "* By using the `canidate schema` we will map the three things we need snorkel to extract, `Stratigraphic description`, `Mineral Information`, and a third function to attempt and extract the `age` from within the document. \n",
    "\n",
    "* `Strat` subclass has three relations, the `name` of the unit, unit `classification` (Super Group, Group, Formation, etc..) and the `target_word` it was matched too. \n",
    "\n",
    "* `Strat age` will be collected independentally of identifying the stratigraphic unit... Could use macrostrat to fill in the age of a given unit. \n",
    "\n",
    "* `Mineral` matches a target word and a mineral word and fills the span between them. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = ('evaporite','potash','soda lake', 'salt deposit')\n",
    "target_wordss = ('evaporite','potash','soda lake', 'salt deposit')\n",
    "target_dict = dict(zip(target_words,target_wordss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mineral_words = {'halite', 'sylvite','hydrohalite','bischofite', 'antacitite','carnallite','tachyhydrite','rhinneite','kainite','aragonite','calcite','thermonatrite','soda','nahcolite','dolomite','anhydrite','bassanite', 'gypsum' , 'kieserite', 'saderite', 'epsomite', 'thernadite', 'mirabillte', 'celestine', 'glaserite', 'vanthoffite', 'bleodite', 'loeweite', 'langbeinite' , 'leonine', 'glauberite', 'syngenite', 'polyhalite', 'northupite', 'burette', 'techie', 'hanksite', 'lautarite', 'kernite', 'borax', 'colemanite' }              \n",
    "mineral_wordss = {'halite', 'sylvite','hydrohalite','bischofite', 'antacitite','carnallite','tachyhydrite','rhinneite','kainite','aragonite','calcite','thermonatrite','soda','nahcolite','dolomite','anhydrite','bassanite', 'gypsum' , 'kieserite', 'saderite', 'epsomite', 'thernadite', 'mirabillte', 'celestine', 'glaserite', 'vanthoffite', 'bleodite', 'loeweite', 'langbeinite' , 'leonine', 'glauberite', 'syngenite', 'polyhalite', 'northupite', 'burette', 'techie', 'hanksite', 'lautarite', 'kernite', 'borax', 'colemanite' }              \n",
    "mineral_dict = dict(zip(mineral_words,mineral_wordss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Fm.': 'Fm.', 'Gp.': 'Gp.', 'Limestone': 'Limestone', 'Group': 'Group', 'Gp': 'Gp', 'Sandstone': 'Sandstone', 'SGp': 'SGp', 'Supergroup': 'Supergroup', 'Bed': 'Bed', 'Fm': 'Fm', 'Member': 'Member', 'Chert': 'Chert', 'Shale': 'Shale', 'Formation': 'Formation', 'Dolostone': 'Dolostone', 'Dolomite': 'Dolomite', 'SGp.': 'SGp.', 'Conglomerate': 'Conglomerate', 'Mbr.': 'Mbr.', 'Mbr': 'Mbr', 'Subgroup': 'Subgroup'}\n"
     ]
    }
   ],
   "source": [
    "## Copied From Jon's Stromatolite Demo 'strat_var'\n",
    "#words indicating stratigraphic names\n",
    "#strat_flags = [\"Group\", \"Formation\", \"Member\", \"Supergroup\", \"Bed\", \"Subgroup\",\"Gp.\", \"Fm.\", \"Mbr.\", \"SGp.\", \"Gp\", \"Fm\", \"Mbr\", \"SGp\"]\n",
    "    \n",
    "#lith_flags = [\"Dolomite\",\"Dolostone\",\"Limestone\",\"Sandstone\",\"Shale\",\"Conglomerate\",\"Chert\"]\n",
    "\n",
    "#strat_flags = strat_flags+lith_flags\n",
    "#strat_flagss = strat_flags\n",
    "#strat_dict = dict(zip(strat_flags,strat_flagss))\n",
    "#print strat_dict              \n",
    "\n",
    "#words indicating an age\n",
    "#age_flags = [\"Ma.\", \"Ga.\", \"Myr.\",\"Ma\", \"Ga\", \"Myr\"]\n",
    "\n",
    "#list of known and troublesome ligatures\n",
    "#weird_strings = [['\\xef\\xac\\x82', 'fl'], ['\\xef\\xac\\x81', 'fi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import candidate_subclass\n",
    "\n",
    "#Strat = candidate_subclass('Strat', ['strat_name', 'strat_unit','target_word'])\n",
    "#Strat_Age = canidate_subclass('Strat_Age',['strat_name','strat_age'])\n",
    "Mineral = candidate_subclass('Mineral' ,['mineral_word','target word'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams are words considered for a given sentence. \n",
    "\n",
    "from snorkel.candidates import Ngrams, CandidateExtractor\n",
    "from snorkel.matchers import DictionaryMatch\n",
    "\n",
    "ngrams         = Ngrams(n_max=20)\n",
    "mineral_matcher = DictionaryMatch(ignore_case=True, d=mineral_dict)\n",
    "target_matcher  = DictionaryMatch(ignore_case=True, d=target_dict)\n",
    "cand_extractor = CandidateExtractor(Mineral, [ngrams, ngrams], [mineral_matcher, target_matcher])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.models import Document\n",
    "docs = session.query(Document).order_by(Document.name).all()\n",
    "\n",
    "train_sents = set()\n",
    "dev_sents   = set()\n",
    "test_sents  = set()\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for x,s in enumerate(doc.sentences):\n",
    "        if  x  <= 68811:\n",
    "            dev_sents.add(s)\n",
    "        elif  x <= 137622 and i > 68811:\n",
    "            test_sents.add(s)\n",
    "        else:\n",
    "            train_sents.add(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing existing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/206435 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running UDF...\n",
      "('Number of candidates:', 0L)\n",
      "Clearing existing...\n",
      "Running UDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 206435/206435 [30:03<00:00, 114.44it/s] \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of candidates:', 32L)\n",
      "Clearing existing...\n",
      "Running UDF...\n",
      "('Number of candidates:', 0L)\n",
      "CPU times: user 27min, sys: 1min 57s, total: 28min 57s\n",
      "Wall time: 30min 5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, sents in enumerate([train_sents, dev_sents, test_sents]):\n",
    "    cand_extractor.apply(sents, split=i)\n",
    "    print(\"Number of candidates:\", session.query(Mineral).filter(Mineral.split == i).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (snorkel)",
   "language": "python",
   "name": "snorkel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
